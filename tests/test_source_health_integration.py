"""Integration tests for source health tracking (v0.9)."""

import pytest
from datetime import datetime, timezone

from sentinel.database.schema import SourceRun
from sentinel.database.source_run_repo import create_source_run, list_recent_runs
from sentinel.database.raw_item_repo import save_raw_item
from sentinel.runners.ingest_external import main as ingest_external_main


def test_fetch_creates_source_run(session):
    """Test that running fetch creates FETCH SourceRun rows."""
    # This test would require mocking the fetcher, which is complex
    # Instead, we test the SourceRun creation directly
    run_group_id = "test-group-1"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Simulate what cmd_fetch does: create a FETCH SourceRun
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="FETCH",
        run_at_utc=now,
        status="SUCCESS",
        status_code=200,
        duration_seconds=1.5,
        items_fetched=10,
        items_new=8,
    )
    
    session.commit()
    
    # Verify the run was created
    runs = list_recent_runs(session, source_id=source_id, phase="FETCH")
    assert len(runs) == 1
    assert runs[0].phase == "FETCH"
    assert runs[0].status == "SUCCESS"
    assert runs[0].status_code == 200
    assert runs[0].items_fetched == 10
    assert runs[0].items_new == 8


def test_ingest_creates_source_run(session):
    """Test that running ingest creates INGEST SourceRun rows with correct counters."""
    run_group_id = "test-group-1"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Create some raw items first
    for i in range(5):
        save_raw_item(
            session,
            source_id=source_id,
            tier="global",
            candidate={
                "canonical_id": f"test-{i}",
                "title": f"Test Item {i}",
                "url": f"https://example.com/{i}",
                "published_at_utc": now,
                "payload": {"title": f"Test Item {i}"},
            },
        )
    
    session.commit()
    
    # Simulate what ingest_external does: create an INGEST SourceRun
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="INGEST",
        run_at_utc=now,
        status="SUCCESS",
        items_processed=5,
        items_suppressed=1,
        items_events_created=4,
        items_alerts_touched=3,
    )
    
    session.commit()
    
    # Verify the run was created
    runs = list_recent_runs(session, source_id=source_id, phase="INGEST")
    assert len(runs) == 1
    assert runs[0].phase == "INGEST"
    assert runs[0].status == "SUCCESS"
    assert runs[0].items_processed == 5
    assert runs[0].items_suppressed == 1
    assert runs[0].items_events_created == 4
    assert runs[0].items_alerts_touched == 3


def test_run_group_id_linking(session):
    """Test that related FETCH and INGEST runs share same run_group_id."""
    run_group_id = "test-group-1"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Create FETCH run
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="FETCH",
        run_at_utc=now,
        status="SUCCESS",
        items_fetched=10,
        items_new=8,
    )
    
    # Create INGEST run with same run_group_id
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="INGEST",
        run_at_utc=now,
        status="SUCCESS",
        items_processed=8,
        items_events_created=7,
        items_alerts_touched=5,
    )
    
    session.commit()
    
    # Verify both runs share the same run_group_id
    fetch_runs = list_recent_runs(session, source_id=source_id, phase="FETCH")
    ingest_runs = list_recent_runs(session, source_id=source_id, phase="INGEST")
    
    assert len(fetch_runs) == 1
    assert len(ingest_runs) == 1
    assert fetch_runs[0].run_group_id == run_group_id
    assert ingest_runs[0].run_group_id == run_group_id
    assert fetch_runs[0].run_group_id == ingest_runs[0].run_group_id


def test_sources_test_creates_runs(session):
    """Test that sources test command creates at least one SourceRun row."""
    # This is tested indirectly through the fact that cmd_sources_test
    # calls create_source_run, which we test above
    # For a full integration test, we'd need to mock the fetcher
    # which is complex, so we test the components separately
    
    run_group_id = "test-group-1"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Simulate what cmd_sources_test does
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="FETCH",
        run_at_utc=now,
        status="SUCCESS",
        items_fetched=5,
        items_new=5,
    )
    
    session.commit()
    
    # Verify run was created
    runs = list_recent_runs(session, source_id=source_id)
    assert len(runs) >= 1
    assert any(r.phase == "FETCH" for r in runs)


def test_ingest_failure_still_writes_source_run(session):
    """Test that ingest failure still writes INGEST SourceRun row with status=FAILURE (v1.0)."""
    run_group_id = "test-group-1"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Simulate what ingest_external does when a source batch fails:
    # It should still create an INGEST SourceRun with status=FAILURE
    create_source_run(
        session,
        run_group_id=run_group_id,
        source_id=source_id,
        phase="INGEST",
        run_at_utc=now,
        status="FAILURE",
        error="Normalization error: invalid JSON",  # Truncated to 1000 chars
        duration_seconds=0.5,
        items_processed=2,  # Some items processed before failure
        items_suppressed=0,
        items_events_created=1,  # One event created before failure
        items_alerts_touched=0,  # No alerts created due to failure
    )
    
    session.commit()
    
    # Verify the failure run was created
    runs = list_recent_runs(session, source_id=source_id, phase="INGEST")
    assert len(runs) == 1
    assert runs[0].phase == "INGEST"
    assert runs[0].status == "FAILURE"
    assert runs[0].error is not None
    assert "error" in runs[0].error.lower() or "normalization" in runs[0].error.lower()
    assert runs[0].items_processed == 2
    assert runs[0].items_events_created == 1
    assert runs[0].items_alerts_touched == 0


def test_ingest_item_failure_creates_source_run(session, mocker):
    """Test that item-level failures don't prevent INGEST SourceRun creation (v1.0)."""
    run_group_id = "test-group-item-failure"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Create two raw items: one valid, one that will fail during normalization
    save_raw_item(
        session,
        source_id=source_id,
        tier="global",
        candidate={
            "canonical_id": "valid-item-1",
            "title": "Valid Item",
            "url": "https://example.com/valid",
            "published_at_utc": now,
            "payload": {"title": "Valid Item"},
        },
    )
    
    # Create an item that will fail during normalization
    # We'll mock normalize_external_event to raise an exception for this item
    save_raw_item(
        session,
        source_id=source_id,
        tier="global",
        candidate={
            "canonical_id": "invalid-item-1",
            "title": "Invalid Item",
            "url": "https://example.com/invalid",
            "published_at_utc": now,
            "payload": {"title": "Invalid Item"},
        },
    )
    
    session.commit()
    
    # Mock normalize_external_event to fail for the second item
    call_count = [0]
    original_normalize = None
    
    def mock_normalize(*args, **kwargs):
        call_count[0] += 1
        if call_count[0] == 2:  # Second call (invalid-item-1)
            raise ValueError("Simulated normalization error for testing")
        # Import here to avoid circular import
        from sentinel.parsing.normalizer import normalize_external_event
        return normalize_external_event(*args, **kwargs)
    
    mocker.patch(
        "sentinel.runners.ingest_external.normalize_external_event",
        side_effect=mock_normalize
    )
    
    # Run ingest_external_main
    stats = ingest_external_main(
        session=session,
        source_id=source_id,
        run_group_id=run_group_id,
        fail_fast=False,
    )
    session.commit()
    
    # Assert: exactly one INGEST SourceRun row exists for this source_id
    runs = list_recent_runs(session, source_id=source_id, phase="INGEST")
    assert len(runs) >= 1, f"Expected at least 1 INGEST SourceRun, got {len(runs)}"
    
    # Find the run with our run_group_id
    our_run = None
    for run in runs:
        if run.run_group_id == run_group_id:
            our_run = run
            break
    
    assert our_run is not None, f"No INGEST SourceRun found with run_group_id={run_group_id}"
    assert our_run.phase == "INGEST"
    assert our_run.run_group_id == run_group_id
    # Status should be SUCCESS (batch completed, item failures are tracked in counters)
    assert our_run.status == "SUCCESS"
    # Counters: 2 items processed (attempted), at least 1 error
    assert our_run.items_processed == 2, f"Expected 2 items processed, got {our_run.items_processed}"
    # At least the valid item should create an event
    assert our_run.items_events_created >= 1, f"Expected at least 1 event created, got {our_run.items_events_created}"
    # Error message should indicate item-level failures
    assert our_run.error is not None, "Expected error message for item-level failures"
    assert "error" in our_run.error.lower() or "1" in our_run.error, f"Error message should mention errors: {our_run.error}"
    
    # Assert: valid item was processed (event/alert created)
    # This verifies pipeline continues after item failure
    assert stats["processed"] >= 1, f"Expected at least 1 item processed, got {stats['processed']}"
    assert stats["errors"] >= 1, f"Expected at least 1 error, got {stats['errors']}"


def test_ingest_fail_fast_still_writes_source_run(session, mocker):
    """Test that --fail-fast writes SourceRun before exiting (v1.0)."""
    run_group_id = "test-group-fail-fast"
    source_id = "test_source"
    now = datetime.now(timezone.utc).isoformat()
    
    # Create a raw item that will cause a batch-level failure
    save_raw_item(
        session,
        source_id=source_id,
        tier="global",
        candidate={
            "canonical_id": "failing-item-1",
            "title": "Failing Item",
            "url": "https://example.com/failing",
            "published_at_utc": now,
            "payload": {"title": "Failing Item"},
        },
    )
    
    session.commit()
    
    # Mock a function that will cause a batch-level exception
    # We'll mock save_event to raise an exception on first call
    call_count = [0]
    
    def mock_save_event(*args, **kwargs):
        call_count[0] += 1
        if call_count[0] == 1:
            raise RuntimeError("Simulated batch-level failure for fail-fast test")
        # Import here to avoid circular import
        from sentinel.database.event_repo import save_event
        return save_event(*args, **kwargs)
    
    mocker.patch(
        "sentinel.runners.ingest_external.save_event",
        side_effect=mock_save_event
    )
    
    # Run ingest_external_main with fail_fast=True
    # This should raise an exception, but SourceRun should be created first
    with pytest.raises(RuntimeError, match="Simulated batch-level failure"):
        ingest_external_main(
            session=session,
            source_id=source_id,
            run_group_id=run_group_id,
            fail_fast=True,
        )
    
    # Commit to ensure SourceRun is persisted
    session.commit()
    
    # Assert: INGEST SourceRun was created before exception was re-raised
    runs = list_recent_runs(session, source_id=source_id, phase="INGEST")
    assert len(runs) >= 1, f"Expected at least 1 INGEST SourceRun, got {len(runs)}"
    
    # Find the run with our run_group_id
    our_run = None
    for run in runs:
        if run.run_group_id == run_group_id:
            our_run = run
            break
    
    assert our_run is not None, f"No INGEST SourceRun found with run_group_id={run_group_id}"
    assert our_run.phase == "INGEST"
    assert our_run.run_group_id == run_group_id
    # Status should be FAILURE (batch-level exception occurred)
    assert our_run.status == "FAILURE"
    assert our_run.error is not None
    assert "failure" in our_run.error.lower() or "error" in our_run.error.lower()
    # Duration should be set (proves SourceRun was created)
    assert our_run.duration_seconds is not None
    assert our_run.duration_seconds >= 0

